{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"unbabel-comet>=2.0.0\" -U","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U huggingface","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken_value = user_secrets.get_secret(\"HF_TOKEN\")\nos.system(f\"huggingface-cli login --token {token_value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nfrom huggingface_hub import HfApi, hf_hub_download\nfrom comet import download_model, load_from_checkpoint\nfrom tqdm import tqdm\n\n# Hugging Face dataset repository details\ndataset_repo = \"cstr/Capybara-de-snippets\"\ndataset_files = [\n    #\"Capybara_de_GPT4.jsonl\",\n    #\"Capybara_de_Claude-3-Opus.jsonl\",\n    #\"Capybara_de_GPT3.5.jsonl\",\n    #\"Capybara_de_deepl.jsonl\",\n    #\"Capybara_de_mixtral.jsonl\",\n    #\"Capybara_de_occiglot.jsonl\",\n    \"Capybara_de_original (english).jsonl\",\n    \"Capybara_de_nbbl.jsonl\",\n    #\"Capybara_de_t5madlad.jsonl\",\n    #\"Capybara_de_discolm.jsonl\"\n]\n\n# Download the dataset files from Hugging Face\nfile_paths = {}\nfor file_name in dataset_files:\n    file_path = hf_hub_download(repo_id=dataset_repo, filename=file_name, repo_type=\"dataset\")\n    file_paths[file_name] = file_path\n    print(f\"Downloaded {file_name} to {file_path}\")\n\n# Download and load the COMET model\nmodel_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\nmodel = load_from_checkpoint(model_path)\n\n# Open the original English file\nwith open(file_paths[\"Capybara_de_original (english).jsonl\"], \"r\", encoding=\"utf-8\") as file:\n    original_data = [json.loads(line) for line in file]\n\n# Process each translation file\nfor translation_file in dataset_files:\n    if translation_file == \"Capybara_de_original (english).jsonl\":\n        continue  # Skip the original English file\n\n    print(f\"Processing {translation_file}...\")\n\n    # Open the translation file\n    with open(file_paths[translation_file], \"r\", encoding=\"utf-8\") as file:\n        translation_data = [json.loads(line) for line in file]\n\n    # Create a new list to store the updated data with scores\n    updated_data = []\n\n    # Iterate over each conversation in the translation data with a progress bar\n    for conv_idx, conv in enumerate(tqdm(translation_data, desc=\"Evaluating conversations\")):\n        updated_conv = {\"source\": conv[\"source\"], \"conversation\": []}\n\n        # Iterate over each turn in the conversation\n        for turn_idx, turn in enumerate(conv[\"conversation\"]):\n            # Get the corresponding turn from the original data\n            original_turn = original_data[conv_idx][\"conversation\"][turn_idx]\n\n            # Prepare the data for COMET evaluation\n            comet_data = [\n                {\n                    \"src\": original_turn[\"input\"],\n                    \"mt\": turn[\"input\"]\n                },\n                {\n                    \"src\": original_turn[\"output\"],\n                    \"mt\": turn[\"output\"]\n                }\n            ]\n\n            # Perform COMET evaluation\n            comet_scores = model.predict(comet_data, batch_size=8, gpus=1)\n\n            # Add the scores to the turn data\n            updated_turn = {\n                \"input\": turn[\"input\"],\n                \"output\": turn[\"output\"],\n                \"input_score\": comet_scores[0],\n                \"output_score\": comet_scores[1]\n            }\n\n            # Append the updated turn to the conversation\n            updated_conv[\"conversation\"].append(updated_turn)\n\n        # Append the updated conversation to the updated data\n        updated_data.append(updated_conv)\n\n    # Save the updated data to a new JSONL file with UTF-8 encoding\n    output_file = f\"{translation_file[:-6]}_scored.jsonl\"\n    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n        for conv in updated_data:\n            file.write(json.dumps(conv, ensure_ascii=False) + \"\\n\")\n\n    # Upload the scored JSONL file to the dataset repository\n    api = HfApi()\n    api.upload_file(\n        path_or_fileobj=output_file,\n        path_in_repo=output_file,\n        repo_id=dataset_repo,\n        repo_type=\"dataset\"\n    )\n    print(f\"Uploaded {output_file} to the dataset repository.\")\n\nprint(\"Evaluation completed. Scored files have been generated and uploaded.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}